"""
Fincaraiz Web Scraping Orchestrator

This orchestrator takes Fincaraiz search URLs generated by the mapper
and scrapes property data from the search results, outputting CSV files.
"""

import csv
import time
import requests
from typing import Dict, List, Any, Optional
from urllib.parse import urljoin, urlparse
from bs4 import BeautifulSoup
import re
from datetime import datetime
import logging

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class FincaraizOrchestrator:
    """Orchestrates web scraping for Fincaraiz property searches"""
    
    def __init__(self, delay_between_requests: float = 1.0, max_retries: int = 3):
        """
        Initialize the Fincaraiz orchestrator
        
        Args:
            delay_between_requests: Delay in seconds between requests
            max_retries: Maximum number of retries for failed requests
        """
        self.delay_between_requests = delay_between_requests
        self.max_retries = max_retries
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })
        
    def scrape_search_results(self, search_url: str) -> Dict[str, Any]:
        """
        Scrape property data from Fincaraiz search results
        
        Args:
            search_url: Fincaraiz search URL generated by mapper
            
        Returns:
            Dictionary containing scraped properties and metadata
        """
        logger.info(f"Scraping search results from: {search_url}")
        
        try:
            # Fetch the search results page
            response = self._fetch_page_with_retry(search_url)
            if not response:
                return {
                    'error': 'Failed to fetch search results page',
                    'url': search_url,
                    'properties': [],
                    'metadata': {}
                }
            
            # Parse the HTML content
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # Extract properties from the page
            properties = self._extract_properties(soup, search_url)
            
            # Extract search metadata
            metadata = self._extract_search_metadata(soup, search_url)
            
            logger.info(f"Successfully scraped {len(properties)} properties")
            
            return {
                'url': search_url,
                'properties': properties,
                'metadata': metadata,
                'scraped_at': datetime.now().isoformat(),
                'success': True
            }
            
        except Exception as e:
            logger.error(f"Error scraping search results: {str(e)}")
            return {
                'error': f'Scraping failed: {str(e)}',
                'url': search_url,
                'properties': [],
                'metadata': {},
                'success': False
            }
    
    def _fetch_page_with_retry(self, url: str) -> Optional[requests.Response]:
        """Fetch page with retry logic"""
        for attempt in range(self.max_retries):
            try:
                logger.info(f"Fetching page (attempt {attempt + 1}/{self.max_retries})")
                response = self.session.get(url, timeout=30)
                response.raise_for_status()
                
                # Add delay between requests
                if self.delay_between_requests > 0:
                    time.sleep(self.delay_between_requests)
                
                return response
                
            except requests.RequestException as e:
                logger.warning(f"Request failed (attempt {attempt + 1}): {str(e)}")
                if attempt == self.max_retries - 1:
                    logger.error(f"All retry attempts failed for URL: {url}")
                    return None
                
                # Exponential backoff
                time.sleep(2 ** attempt)
        
        return None
    
    def _extract_properties(self, soup: BeautifulSoup, base_url: str) -> List[Dict[str, Any]]:
        """Extract property data from search results page"""
        properties = []
        
        # Look for property cards/containers
        # Fincaraiz typically uses specific CSS classes for property listings
        property_containers = soup.find_all(['div', 'article'], class_=re.compile(r'property|listing|card|item'))
        
        if not property_containers:
            # Try alternative selectors
            property_containers = soup.find_all('div', class_=re.compile(r'result|search|list'))
        
        logger.info(f"Found {len(property_containers)} potential property containers")
        
        for container in property_containers:
            try:
                property_data = self._extract_single_property(container, base_url)
                if property_data:
                    properties.append(property_data)
            except Exception as e:
                logger.warning(f"Error extracting property data: {str(e)}")
                continue
        
        return properties
    
    def _extract_single_property(self, container, base_url: str) -> Optional[Dict[str, Any]]:
        """Extract data from a single property container"""
        try:
            property_data = {}
            
            # Extract basic property information
            property_data['title'] = self._extract_text(container, ['h1', 'h2', 'h3', 'h4'], class_=re.compile(r'title|name|heading'))
            property_data['price'] = self._extract_price(container)
            property_data['address'] = self._extract_text(container, ['span', 'div', 'p'], class_=re.compile(r'address|location|ubicacion'))
            property_data['area'] = self._extract_area(container)
            property_data['bedrooms'] = self._extract_bedrooms(container)
            property_data['bathrooms'] = self._extract_bathrooms(container)
            property_data['parking'] = self._extract_parking(container)
            property_data['stratum'] = self._extract_stratum(container)
            
            # Extract property type and operation
            property_data['property_type'] = self._extract_property_type(container)
            property_data['operation'] = self._extract_operation(container)
            
            # Extract amenities
            property_data['amenities'] = self._extract_amenities(container)
            
            # Extract contact information
            property_data['contact'] = self._extract_contact(container)
            
            # Extract property URL
            property_data['property_url'] = self._extract_property_url(container, base_url)
            
            # Extract images
            property_data['images'] = self._extract_images(container, base_url)
            
            # Calculate price per m²
            property_data['price_per_m2'] = self._calculate_price_per_m2(property_data)
            
            # Extract additional property details
            property_data['floor'] = self._extract_floor(container)
            property_data['construction_age'] = self._extract_construction_age(container)
            property_data['construction_year'] = self._extract_construction_year(container)
            property_data['interior_exterior'] = self._extract_interior_exterior(container)
            property_data['finish_quality'] = self._extract_finish_quality(container)
            property_data['conservation_state'] = self._extract_conservation_state(container)
            property_data['location_quality'] = self._extract_location_quality(container)
            property_data['administration'] = self._extract_administration(container)
            property_data['deposit'] = self._extract_deposit(container)
            property_data['terrace_area'] = self._extract_terrace_area(container)
            property_data['walking_closet'] = self._extract_walking_closet(container)
            property_data['loft'] = self._extract_loft(container)
            property_data['study_room'] = self._extract_study_room(container)
            property_data['elevator'] = self._extract_elevator(container)
            property_data['mezanine'] = self._extract_mezanine(container)
            property_data['cocina'] = self._extract_cocina(container)
            property_data['bodega'] = self._extract_bodega(container)
            property_data['property_code'] = self._extract_property_code(container)
            
            # Only return if we have at least basic information
            if property_data.get('title') or property_data.get('price'):
                return property_data
            
        except Exception as e:
            logger.warning(f"Error extracting single property: {str(e)}")
        
        return None
    
    def _calculate_price_per_m2(self, property_data: Dict[str, Any]) -> float:
        """Calculate price per m² from price and area data"""
        try:
            # Extract price value
            price_text = property_data.get('price', '')
            if not price_text:
                return 0.0
            
            # Clean price text and extract numeric value
            price_clean = re.sub(r'[^\d]', '', price_text)
            if not price_clean:
                return 0.0
            
            price_value = float(price_clean)
            
            # Extract area value
            area_text = property_data.get('area', '')
            if not area_text:
                return 0.0
            
            # Extract numeric value from area (e.g., "50 m²" -> 50)
            area_match = re.search(r'(\d+(?:\.\d+)?)', area_text)
            if not area_match:
                return 0.0
            
            area_value = float(area_match.group(1))
            
            # Calculate price per m²
            if area_value > 0:
                return round(price_value / area_value, 2)
            else:
                return 0.0
                
        except (ValueError, TypeError, AttributeError) as e:
            logger.warning(f"Error calculating price per m²: {str(e)}")
            return 0.0
    
    def _extract_text(self, container, tags: List[str], class_: re.Pattern = None) -> Optional[str]:
        """Extract text from container using specified tags and class pattern"""
        for tag in tags:
            elements = container.find_all(tag, class_=class_) if class_ else container.find_all(tag)
            for element in elements:
                text = element.get_text(strip=True)
                if text:
                    return text
        return None
    
    def _extract_price(self, container) -> Optional[str]:
        """Extract price information"""
        # Look for price patterns - updated to handle spaces and different formats
        price_patterns = [
            r'\$\s*[\d.,]+',  # $ 1.365.773
            r'\$\s*[\d.,]+\+',  # $ 1.365.773+
            r'[\d.,]+\s*pesos',
            r'[\d.,]+\s*COP',
            r'[\d.,]+\s*millones?',  # 1.5 millones
            r'[\d.,]+\s*M'  # 1.5M
        ]
        
        text = container.get_text()
        for pattern in price_patterns:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                return match.group(0)
        
        return None
    
    def _extract_area(self, container) -> Optional[str]:
        """Extract area information"""
        area_patterns = [
            r'(\d+)\s*m²',
            r'(\d+)\s*metros',
            r'(\d+)\s*m2'
        ]
        
        text = container.get_text()
        for pattern in area_patterns:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                return f"{match.group(1)} m²"
        
        return None
    
    def _extract_bedrooms(self, container) -> Optional[int]:
        """Extract number of bedrooms"""
        bedroom_patterns = [
            r'(\d+)\s*habitaciones?',
            r'(\d+)\s*alcobas?',
            r'(\d+)\s*cuartos?'
        ]
        
        text = container.get_text()
        for pattern in bedroom_patterns:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                return int(match.group(1))
        
        return None
    
    def _extract_bathrooms(self, container) -> Optional[float]:
        """Extract number of bathrooms"""
        bathroom_patterns = [
            r'(\d+(?:\.\d+)?)\s*baños?',
            r'(\d+(?:\.\d+)?)\s*banos?'
        ]
        
        text = container.get_text()
        for pattern in bathroom_patterns:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                return float(match.group(1))
        
        return None
    
    def _extract_parking(self, container) -> Optional[int]:
        """Extract parking information"""
        parking_patterns = [
            r'(\d+)\s*parqueaderos?',
            r'(\d+)\s*garajes?'
        ]
        
        text = container.get_text()
        for pattern in parking_patterns:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                return int(match.group(1))
        
        return None
    
    def _extract_stratum(self, container) -> Optional[int]:
        """Extract stratum information"""
        stratum_patterns = [
            r'estrato\s*(\d+)',
            r'stratum\s*(\d+)'
        ]
        
        text = container.get_text()
        for pattern in stratum_patterns:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                return int(match.group(1))
        
        return None
    
    def _extract_property_type(self, container) -> Optional[str]:
        """Extract property type"""
        property_types = ['apartamento', 'casa', 'apartaestudio', 'oficina', 'local']
        text = container.get_text().lower()
        
        for prop_type in property_types:
            if prop_type in text:
                return prop_type.title()
        
        return None
    
    def _extract_operation(self, container) -> Optional[str]:
        """Extract operation type (rent/sale)"""
        text = container.get_text().lower()
        
        if 'arriendo' in text or 'renta' in text:
            return 'ARRIENDO'
        elif 'venta' in text or 'sale' in text:
            return 'VENTA'
        
        return None
    
    def _extract_amenities(self, container) -> List[str]:
        """Extract amenities list"""
        amenities = []
        amenity_keywords = [
            'ascensor', 'balcón', 'jardín', 'piscina', 'gimnasio', 'vigilancia',
            'parqueadero', 'depósito', 'closet', 'estudio', 'loft', 'terraza'
        ]
        
        text = container.get_text().lower()
        for amenity in amenity_keywords:
            if amenity in text:
                amenities.append(amenity.title())
        
        return amenities
    
    def _extract_contact(self, container) -> Optional[str]:
        """Extract contact information"""
        contact_patterns = [
            r'(\d{3}[-.\s]?\d{3}[-.\s]?\d{4})',  # Phone numbers
            r'([a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,})'  # Email
        ]
        
        text = container.get_text()
        for pattern in contact_patterns:
            match = re.search(pattern, text)
            if match:
                return match.group(0)
        
        return None
    
    def _extract_property_url(self, container, base_url: str) -> Optional[str]:
        """Extract property detail URL"""
        link = container.find('a', href=True)
        if link:
            href = link['href']
            if href.startswith('http'):
                return href
            else:
                return urljoin(base_url, href)
        return None
    
    def _extract_images(self, container, base_url: str) -> List[str]:
        """Extract property images"""
        images = []
        img_tags = container.find_all('img')
        
        for img in img_tags:
            src = img.get('src') or img.get('data-src')
            if src:
                if src.startswith('http'):
                    images.append(src)
                else:
                    images.append(urljoin(base_url, src))
        
        return images
    
    def _extract_search_metadata(self, soup: BeautifulSoup, url: str) -> Dict[str, Any]:
        """Extract metadata about the search results"""
        metadata = {
            'total_results': 0,
            'search_filters': {},
            'page_info': {}
        }
        
        # Try to extract total results count
        results_text = soup.get_text()
        results_match = re.search(r'(\d+)\s*resultados?', results_text, re.IGNORECASE)
        if results_match:
            metadata['total_results'] = int(results_match.group(1))
        
        # Extract search filters from URL
        parsed_url = urlparse(url)
        path_parts = parsed_url.path.strip('/').split('/')
        
        if len(path_parts) >= 2:
            metadata['search_filters'] = {
                'operation': path_parts[0] if path_parts[0] in ['venta', 'arriendo'] else None,
                'property_type': path_parts[1] if path_parts[1] in ['apartamentos', 'casas', 'apartaestudios'] else None
            }
        
        return metadata
    
    def save_to_csv(self, properties: List[Dict[str, Any]], filename: str) -> bool:
        """
        Save scraped properties to CSV file
        
        Args:
            properties: List of property dictionaries
            filename: Output CSV filename
            
        Returns:
            True if successful, False otherwise
        """
        if not properties:
            logger.warning("No properties to save")
            return False
        
        try:
            # Get all unique keys from all properties
            all_keys = set()
            for prop in properties:
                all_keys.update(prop.keys())
            
            # Define CSV fieldnames in a logical order
            fieldnames = [
                'title', 'price', 'address', 'area', 'bedrooms', 'bathrooms',
                'parking', 'stratum', 'property_type', 'operation', 'amenities',
                'contact', 'property_url', 'images'
            ]
            
            # Add any additional keys not in the predefined list
            for key in sorted(all_keys):
                if key not in fieldnames:
                    fieldnames.append(key)
            
            with open(filename, 'w', newline='', encoding='utf-8') as csvfile:
                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
                writer.writeheader()
                
                for prop in properties:
                    # Convert lists to strings for CSV
                    row = {}
                    for key, value in prop.items():
                        if isinstance(value, list):
                            row[key] = '; '.join(str(item) for item in value)
                        else:
                            row[key] = value
                    writer.writerow(row)
            
            logger.info(f"Successfully saved {len(properties)} properties to {filename}")
            return True
            
        except Exception as e:
            logger.error(f"Error saving to CSV: {str(e)}")
            return False
    
    def process_search_url(self, search_url: str, output_filename: str = None) -> Dict[str, Any]:
        """
        Complete process: scrape search results and save to CSV
        
        Args:
            search_url: Fincaraiz search URL
            output_filename: Optional output CSV filename
            
        Returns:
            Dictionary with processing results
        """
        # Scrape the search results
        scrape_result = self.scrape_search_results(search_url)
        
        if not scrape_result.get('success', False):
            return scrape_result
        
        # Generate output filename if not provided
        if not output_filename:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_filename = f"fincaraiz_properties_{timestamp}.csv"
        
        # Save to CSV
        csv_success = self.save_to_csv(scrape_result['properties'], output_filename)
        
        return {
            **scrape_result,
            'csv_file': output_filename if csv_success else None,
            'csv_success': csv_success
        }
    
    def process_multiple_search_urls(self, search_urls: List[str], output_filename: str = None) -> Dict[str, Any]:
        """
        Process multiple search URLs and combine results into a single CSV
        
        Args:
            search_urls: List of Fincaraiz search URLs
            output_filename: Optional output CSV filename
            
        Returns:
            Dictionary with processing results
        """
        all_properties = []
        successful_urls = []
        failed_urls = []
        
        for i, search_url in enumerate(search_urls):
            logger.info(f"Processing URL {i+1}/{len(search_urls)}: {search_url}")
            
            # Scrape the search results
            scrape_result = self.scrape_search_results(search_url)
            
            if scrape_result.get('success', False):
                properties = scrape_result.get('properties', [])
                all_properties.extend(properties)
                successful_urls.append(search_url)
                logger.info(f"Successfully scraped {len(properties)} properties from URL {i+1}")
            else:
                failed_urls.append({
                    'url': search_url,
                    'error': scrape_result.get('error', 'Unknown error')
                })
                logger.warning(f"Failed to scrape URL {i+1}: {scrape_result.get('error', 'Unknown error')}")
            
            # Add delay between requests
            if i < len(search_urls) - 1:  # Don't delay after the last request
                time.sleep(self.delay_between_requests)
        
        # Generate output filename if not provided
        if not output_filename:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_filename = f"fincaraiz_properties_multiple_{timestamp}.csv"
        
        # Save combined results to CSV
        csv_success = self.save_to_csv(all_properties, output_filename)
        
        return {
            'success': len(successful_urls) > 0,
            'properties': all_properties,
            'total_properties': len(all_properties),
            'successful_urls': successful_urls,
            'failed_urls': failed_urls,
            'csv_file': output_filename if csv_success else None,
            'csv_success': csv_success,
            'urls_processed': len(search_urls),
            'urls_successful': len(successful_urls),
            'urls_failed': len(failed_urls)
        }
    
    def _extract_floor(self, container) -> Optional[int]:
        """Extract floor number"""
        floor_patterns = [
            r'(\d+)\s*piso',
            r'piso\s*(\d+)',
            r'(\d+)\s*floor'
        ]
        
        text = container.get_text()
        for pattern in floor_patterns:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                try:
                    return int(match.group(1))
                except ValueError:
                    continue
        return None
    
    def _extract_construction_age(self, container) -> Optional[int]:
        """Extract construction age in years"""
        age_patterns = [
            r'(\d+)\s*años?',
            r'construcción\s*(\d+)',
            r'antigüedad\s*(\d+)'
        ]
        
        text = container.get_text()
        for pattern in age_patterns:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                try:
                    return int(match.group(1))
                except ValueError:
                    continue
        return None
    
    def _extract_construction_year(self, container) -> Optional[int]:
        """Extract construction year"""
        year_patterns = [
            r'construcción\s*(\d{4})',
            r'año\s*(\d{4})',
            r'(\d{4})\s*construcción'
        ]
        
        text = container.get_text()
        for pattern in year_patterns:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                try:
                    year = int(match.group(1))
                    if 1900 <= year <= 2030:  # Reasonable year range
                        return year
                except ValueError:
                    continue
        return None
    
    def _extract_interior_exterior(self, container) -> Optional[str]:
        """Extract interior/exterior classification"""
        text = container.get_text().lower()
        if 'interior' in text:
            return 'I'
        elif 'exterior' in text:
            return 'E'
        return None
    
    def _extract_finish_quality(self, container) -> Optional[int]:
        """Extract finish quality rating (1-5)"""
        # This would need specific patterns for quality indicators
        # For now, return a default value
        return 4
    
    def _extract_conservation_state(self, container) -> Optional[int]:
        """Extract conservation state rating (1-5)"""
        # This would need specific patterns for conservation indicators
        # For now, return a default value
        return 4
    
    def _extract_location_quality(self, container) -> Optional[int]:
        """Extract location quality rating (1-5)"""
        # This would need specific patterns for location indicators
        # For now, return a default value
        return 5
    
    def _extract_administration(self, container) -> Optional[float]:
        """Extract administration fee"""
        admin_patterns = [
            r'administración\s*\$?\s*([\d.,]+)',
            r'admin\s*\$?\s*([\d.,]+)',
            r'administración\s*([\d.,]+)'
        ]
        
        text = container.get_text()
        for pattern in admin_patterns:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                try:
                    admin_str = match.group(1).replace(',', '')
                    return float(admin_str)
                except ValueError:
                    continue
        return None
    
    def _extract_deposit(self, container) -> Optional[bool]:
        """Extract deposit availability"""
        text = container.get_text().lower()
        return 'depósito' in text or 'deposito' in text
    
    def _extract_terrace_area(self, container) -> Optional[float]:
        """Extract terrace area"""
        terrace_patterns = [
            r'terraza\s*(\d+(?:\.\d+)?)\s*m²',
            r'balcón\s*(\d+(?:\.\d+)?)\s*m²',
            r'(\d+(?:\.\d+)?)\s*m²\s*terraza'
        ]
        
        text = container.get_text()
        for pattern in terrace_patterns:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                try:
                    return float(match.group(1))
                except ValueError:
                    continue
        return None
    
    def _extract_walking_closet(self, container) -> Optional[bool]:
        """Extract walking closet availability"""
        text = container.get_text().lower()
        return 'walking closet' in text or 'closet' in text
    
    def _extract_loft(self, container) -> Optional[bool]:
        """Extract loft availability"""
        text = container.get_text().lower()
        return 'loft' in text
    
    def _extract_study_room(self, container) -> Optional[bool]:
        """Extract study room availability"""
        text = container.get_text().lower()
        return 'estudio' in text or 'sala tv' in text or 'study' in text
    
    def _extract_elevator(self, container) -> Optional[bool]:
        """Extract elevator availability"""
        text = container.get_text().lower()
        return 'ascensor' in text or 'elevator' in text
    
    def _extract_mezanine(self, container) -> Optional[bool]:
        """Extract mezanine availability"""
        text = container.get_text().lower()
        return 'mezanine' in text or 'mezzanine' in text
    
    def _extract_cocina(self, container) -> Optional[bool]:
        """Extract kitchen availability"""
        text = container.get_text().lower()
        return 'cocina' in text or 'kitchen' in text
    
    def _extract_bodega(self, container) -> Optional[bool]:
        """Extract warehouse/storage availability"""
        text = container.get_text().lower()
        return 'bodega' in text or 'warehouse' in text
    
    def _extract_property_code(self, container) -> Optional[str]:
        """Extract property code from URL or title"""
        # Look for property URL first
        url_element = container.find('a', href=True)
        if url_element:
            url = url_element.get('href', '')
            # Extract code from URL (e.g., last part of URL)
            code_match = re.search(r'/(\d+)/?$', url)
            if code_match:
                return code_match.group(1)
        
        # Fallback to title
        title = self._extract_text(container, ['h1', 'h2', 'h3', 'h4'], class_=re.compile(r'title|name|heading'))
        if title:
            # Extract numbers from title
            code_match = re.search(r'(\d+)', title)
            if code_match:
                return code_match.group(1)
        
        return None


if __name__ == "__main__":
    # Test the orchestrator
    orchestrator = FincaraizOrchestrator()
    
    # Test URL (you can replace with actual Fincaraiz search URL)
    test_url = "https://www.fincaraiz.com.co/arriendo/apartamentos/bogota/1-o-mas-habitaciones/1-o-mas-banos/desde-1125000/hasta-1875000/m2-desde-40/m2-hasta-60/edificados/publicado-ultimos-7-dias?IDmoneda=4&stratum[]=3"
    
    print("Testing Fincaraiz Orchestrator...")
    result = orchestrator.process_search_url(test_url, "test_fincaraiz_properties.csv")
    
    print(f"Success: {result.get('success', False)}")
    print(f"Properties found: {len(result.get('properties', []))}")
    print(f"CSV file: {result.get('csv_file', 'Not saved')}")
    
    if result.get('error'):
        print(f"Error: {result['error']}")
